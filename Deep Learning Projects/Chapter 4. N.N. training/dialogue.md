# Chapter 4. 신경망 학습

챕터 4, / 신경망 학습에 관한 내용입니다. / 
데이터를 통한 학습을 시작으로 / 손실 함수에 대해 이야기하고 / 수치 미분과 이를 통해 만들어지는 기울기에 대해 말한 다음 / 신경망 학습에 대한 전체적인 구성에 대해 / 정리하도록 하겠습니다.

## 4.1 데이터에서 학습한다.

앞선 퍼셉트론과 신경망의 대표적인 차이는 / 퍼셉트론은 가중치를 수동으로 조절하고 / 신경망은 데이터를 보고 컴퓨터가 계산한다는 데 있습니다.

### 4.1.1 데이터 주도 학습

처음으로 돌아가서 / 인간의 문제 풀이 방식에는 / 경험과 직관이라는 접근 방식이 있습니다. / 
그러나 매번 사람이 / 많은 양의 데이터를 훑고 / 초기 접근 방식을 찾을 순 없습니다. / 
(맞닥뜨린 상황에 대한 / 경험 부족이 이유일 수도 있고, / 단순히 데이터가 많기 때문일 수도 있습니다.)

그래서 경험과 직관 이외에 / 사람이 문제에 접근하는 방식 중 하나인 / 규칙 찾기를 통해 / 데이터에 대해 접근하고자 했습니다. / 
마치 5는 / 중간 아래쪽에 타원에 가까운 곡선이 있고, / 왼쪽 아래와 오른쪽 위에 선에 끝이 있는 등의 특징으로 알아볼 수 있죠. / 
여기서 / 분류할 특징을 사람이 정하고 / 지도 학습의 대표적인 분류 방법인 SVM, KNN 등을 통해 / 모형을 분리할 수 있습니다.

신경망은 / 여기서 더 나아가 데이터의 특징도 기계가 특정하게 합니다. / 
결과적으로 신경망은 / 주어진 데이터만을 활용해 / 처음부터 끝까지 학습할 수 있습니다.

### 4.1.2 훈련 데이터와 시험 데이터

그렇다면 / 이렇게 만들어진 모형이 제대로 구성되었음을 / 어떻게 확인하고 평가할 수 있을까요?

쉽게 떠올리는 방법이 / 테스트 데이터를 이용해 / 적중률을 확인하는 것일 겁니다.

그러나 새로운 데이터를 얻는 건 / 비용과 시간이 추가로 발생하는 것이고 / 학습에 이용한 데이터로 테스트하는 건 / 제대로된 모형 평가가 되지 않습니다.
우리가 만들고자 하는 모델은 / 학습에 이용한 데이터 말고 / 다른 데이터에도 제대로 작동(분류, 판단)해야 합니다. / 
그런데 훈련 데이터를 테스트 데이터로 하면 / 이러한 범용 능력을 / 평가할 수 없습니다. / 

만약 모델이 / 학습 데이터를 너무 잘 설명한다면 / 학습 데이터에 없는 경우는 / 제대로 설명하지 못할 겁니다. / 
이렇게 하나의 데이터셋, / 학습에 이용한 데이터셋에 지나치게 높은 학습률, / 다시 지나치게 낮은 오류율을 / 가지는 상태를 과대적합이라고 합니다. / 과대적합이 일어나면 앞서 말한 것처럼 / 학습에 이용된 데이터와 다른 데이터에서는 / 모델이 기대와 다른 값을 출력할 수도 있습니다.

그래서 학습용 데이터의 일부를 / 테스트 데이터로 사용합니다. / 
테스트 데이터로 분리된 데이터는 과대적합을 이유로 / 학습에 사용되지 않습니다.

---

## 4.2 손실 함수

지금부터 출력으로 예측된 값은 / 원-핫 인코딩을 통해 변환된 값을 이용합니다. / 
원-핫 인코딩은 n번째 클래스를 가리키는 값, / n을 행렬에서 실제로 그 위치를 가리키도록 / 벡터로 변환하는 것을 말합니다. / 
예를 들어 4번째 클래스는 / 인덱스 0부터 시작하는 벡터에서 / 인덱스 4에서만 1의 값을 갖도록 / 변환됩니다.

앞서 데이터를 통해 학습한다는 의미는 / 신경망을 이루는 노드들의 / 가중치와 편향을 / 컴퓨터가 계산하는 거라고 했습니다. / 
이는 다시 말해 / 수치적으로 최적화된 값을 / 컴퓨터로 계산한다는 것이고, / 이때 이를 계산하고 최적에 대한 평가 지표가 되는 함수를 / 손실 함수라고 합니다.

### 4.2.1 오차제곱합

오차에 대한 수치적 지표로 사용되고, / 선형식의 최적화된 모수를 찾는 방법 중, / 가장 대표적인 것으로 / 오차제곱합이 있습니다. / 
이는 모델에 의해 예측된 값과 / 실제값의 차이를 제곱해 모두 합해서 구합니다. / 
오차제곱합은 제곱을 사용해서 / 실제값과 예측값의 차이에 / 크게 반응하는 것이 특징입니다.

이를 이런 식으로 numpy 패키지의 행렬, 행렬 연산을 이용해 구현할 수 있습니다.

### 4.2.2 교차 엔트로피 오차

다른 것으로는 / 교차 엔트로피 오차라는 것이 있습니다. / 
t는 오차제곱합에서처럼 실제 클래스 값이 / 원-핫 인코딩을 통해 변환된 값입니다. / 
오차제곱합이 모든 클래스에 대해서 / 실제값과 해당 클래스일 확률에 대한 오차를 합한 거라면 / 교차 엔트로피 오차는 / 실제 클래스일 확률만을 가지고 / 값을 계산합니다.

다시, / 출력 클래스가 A, B, C로 세개이고 / 어떤 입력에 대해 각 클래스일 확률이 0.2, 0.7, 0.1이고 / 해당 데이터의 실제 클래스가 A라면 / 로그연산에 따라 0.2만을 가지고 손실 함수값을 계산하게 됩니다.

교차 엔트로피 오차는 코드로 이렇게 표현합니다. / 
자연로그로의 계산 이전에 / 더해주는 델타는 / 입력되는 값이 0으로 갈수록 /마이너스 무한대로 발산하는 것을 방지하고자 / 더하는 작은 값입니다. / 
앞서와 마찬가지로 numpy 패키지의 행렬, 행렬 연산을 이용한 구현입니다. / 
여기까지가 대표적인 손실 함수에 대한 내용입니다.

### 4.2.3 미니배치 학습

신경망이 추구하는 최적화는 / 손실 함수의 값이 작은 매개변수들(가중치, 편향)을 찾는 것입니다. / 
손실 함수의 값이 작다는 건 / 예측된 값이 실제값의 차이가 적다는 걸 의미하죠.

최적의 값을 찾는 가장 단순한 방법은 / 모든 경우에서의 손실 함수 값을 계산해 비교하는 것입니다. / 
앞선 손실 함수들은 / 하나의 경우(한 쌍의 가중치, 편향)에서 / 하나의 데이터에 대한 계산식이었습니다. / 
따라서 입력 데이터의 수가 N개라면 / N번을 해야 한 쌍의 가중치, 편향에 대한 손실 함수 값을 구해야 합니다. / 
이렇게 모든 경우를 계산하는 건 / 아무리 사람의 간섭을 줄이고 컴퓨터만 돌린다 해도 시간을 낭비하는 것입니다.

그래서 데이터 중, / 일부를 골라 학습을 진행합니다.
이때 선택되는 일부를 미니배치라 부르고 / 훈련 데이터 중 일부만을 가지고 학습을 진행하는 것을 / 미니배치 학습이라 합니다.

### 4.2.5 왜 손실 함수를 설정하는가?

그렇다면 왜 손실 함수를 설정하는 걸까요? / 
처음으로 돌아가서 / 신경망 학습에서 최적의 값을 찾는 건 / 정확도가 높은 모델을 구성하기 위함이고, / 가중치의 최적의 값을 찾고자 / 오차에 대한 수치 지표를 미분해 이용합니다. / 
미분을 이용하는 이유는 / 미분은 함수에서 입력에 따른 출력값에 변화를 나타내고, / 다시 말해 / 가중치 값의 변화에 따른 / 수치 지표 값의 변화를 나타내기 때문입니다.

그런데 정확도가 지표가 되면 / 미분된 함수값이 0인 지점이 너무 많아집니다. / 
지도 학습의 분류를 예로 들자면, / 출력은 소프트맥스를 통한 확률값으로 나오고, / 이 중 가장 높은 확률의 클래스로 예측됩니다. / 
따라서 출력값은 1, 5, 4, 2 등의 원-핫 인코딩 결과값으로 연속적이지 않습니다. / 
당연히 이를 이용해 계산하는 / 정확도 또한 연속적인 함수가 아닙니다. / 
이는 정확도가 가중치에 대한 / 미분 가능한 함수가 아니라는 것이고 / 다시 이는 모형 학습 시 / 모형이 최적을 향해 제대로 움직이고 있는지 / 확인할 수 없게 합니다.

그래서 이를 뒤집어 생각한 겁니다. / 
정확도를 높이는 것에서 오차를 줄이는 것으로. / 
그것도 손실 함수라는 연산을 거쳐 / 수치 지표의 값을 정수 혹은 / 불연속적이던 실수에서 / 연속적인 실수로 만듭니다. / 
더불어 계단함수보다 / 시그모이드의 사용을 선호하는 이유도 / 같은 맥락입니다.

---

## 4.3 수치 미분 (Numerical Differentiation)

### 4.3.1 미분

앞서 미분은 / f(x)에서 x값에 의한 출력의 변화 정도로 표현했는데요, / 다시 말하자면 미분은 특정 순간에 / 출력의 변화 정도를 나타내고 / 이와 같은 수식을 기반으로 합니다. / 
h의 값이 굉장히 작아지면서 / x라는 순간의 변화율을 / 수치적으로 확인할 수 있습니다.

이를 코드로 구현하면 / 이런 형식이 됩니다. / 
이 코드에서 주목해야 할 점이 2가지 있습니다. /
앞에 식에서도 볼 수 있는데, / 리미트 항을 보면 / 마치 x + h와 x 지점의 기울기를 구하는 것이 되므로 / h가 아무리 작은 값이라지만, / 원래 구하려면 x의 순간변화율을 구하는 것은 / 실제로 아니게 됩니다. /

그래서 다음과 같이 나머지 함수에도 / h만큼 움직이게 해 / 의도적으로 중앙을 x로 맞춥니다.

나머지 하나는 순간을 만들어줄 h에 관한 것입니다. / 
h에 더 작은 값을 부여할 수도 있지만, / 프로그래밍에는 반올림 오차라는 것이 존재해 / 꽤 작은 값 이하의 값은 / 표기 등을 이유로 0과 같다고 인식해 / 더 작은 값을 쓸 때는 조심해야 합니다.

이렇게 / 아주 작은 차분(차이)으로 미분하는 것, / 미분의 기본 개념에 따른 근사치 계산을 수치 미분이라 하고 / 이론적으로 미분값을 계산하는 것을 해석적 미분이라고 합니다. / 
이 둘의 값은 크게 차이가 나지 않습니다.

### 4.3.3 편미분

지금까지 했던 앞선 미분에선 / 변수가 하나였습니다. 지금부터 소개할 편미분은 / 식을 구성하는 변수가 / 하나가 아닐 때의 미분을 말합니다. / 
앞선 미분과 크게 달라지지 않지만, / 미분하고자 하는 대상만을 변수로 취급하고 / 나머지는 상수 취급합니다. / 
아래 코드를 보면 / x0^2 + x1^2으로 이루어진 function_2를 / 각 변수에 대해 (4, 3) 지점의 미분값을 구하고자 하면 / x0에 대한 미분은 x1을 상수 취급하므로 tmp1 함수와 같은 모양새가 되고, / x1에 대한 미분은 x0를 상수 취급하므로 / tmp2 함수와 같은 형태가 됩니다. / 

---

## 4.4 기울기

편미분을 통해 둘 이상의 변수가 있는 함수도 미분할 수 있었습니다. / 여기서 한 발 더 나아가 / 편의를 위해 / 한 번에 하나 이상의 변수로 / 각각 미분을 진행해보려고 합니다. / 
이때 미분된 값을 행렬로 묶어 표현한 것을 / 기울기라 하고 gradient라는 표현을 씁니다. / 
기울기는 코드로 다음과 같이 표현할 수 있습니다.

numpy 패키지에 zeros_like 함수를 통해 / x와 같은 형태를 만들어두고, / for문으로 x 이번 위치에 대한 수치 미분값을 구해 / 하나의 변수로 반환하는 형태입니다. / 
따라서 손실 함수에 대한 편미분 기울기는 / 가중치의 각 원소들로 / 편미분을 진행하기 때문에 가중치와 형태가 같습니다.

### 4.4.1 경사법(경사하강법)

기계학습과 신경망은 / 학습 단계에서 최적의 가중치를 찾아야 합니다. / 
여기서 최적이란 손실 함수의 최솟값을 의미하고 / 이는 미분값을 통해 찾을 수 있습니다.

함수의 최솟값은 함수의 형태를 보고 구할 수도 있지만, / 이를 미분을 통해 수치적으로 구할 수 있습니다. / 
미분값이 0이 되는 지점은 / 원래 함수의 변화량이 0이 되는 지점이고, / 그 중에서 미분값 부호가 반전되는 지점은 원래 함수에서 최댓값 혹은 회솟값을 찍고 / 값이 반전되는 지점을 의미합니다. / 

이렇게 미분, 기울기를 통해 / 손실 함수의 최솟값을 찾아가는 방법을 / 경사하강법이라 합니다. / 
다시 경사하강법은 초기 지점의 기울기를 보고 / 일정 만큼 움직이고, / 다시 기울기를 확인하고 움직이고를 반복하면서 / 손실 함수의 값을 점차 줄이는 것입니다.
이와 유사하지만, 손실 함수의 부호를 반전시켜 / 손실 함수의 최댓값을 구하기도 합니다. / 
부호의 반전 이외에 딱히 달라지는 것은 없지만, / 최솟값이 아닌 최댓값을 찾는다고 해서 / 경사 상승법이라고 부릅니다. / 

앞서 얘기한 경사하강법을 코드로 표현하면 이렇게 됩니다. / 
여기서 기울기를 확인하고 / 움직이는 크기를 / 학습률이라고 합니다. / 
학습률이 크면 크게 움직일 것이고, / 작으면 아주 조금 값을 갱신하겠죠.

여기서 앞서 정의한 function_2를 통해 / 학습률만 다른 경사하강법을 진행해보면 / 학습률이 너무 작으면 / 가중치가 초기값에서 크게 변화하지 않고 끝나며, / 너무 크면 / 최솟값으로 수혐하지 못하고 끝난다는 것을 알 수 있습니다.

---

## 4.5 학습 알고리즘 구현하기

지금까지 신경망에는 / 입력층, 출력층 말고도 은닉층이 존재하고, / 이 은닉층은 가중치와 편향이 이용되는 공간이었습니다. / 
이 신경망에 대해 전체적인 흐름을 살펴보자면 / 모든 데이터를 학습하는 것은 비효율적입니다, / 그래서 임의의 표본, 미니 배치를 통해 모형 학습을 진행합니다. / 
최적의 값은 손실함수의 최솟값을 찾는 것으로 귀결되며 / 손실함수의 편미분을 통해 얻은 기울기를 이용합니다. / 
이번 단계의 기울기 계산을 마치면 / 학습률과의 곱에 따라 기울기의 방향으로 이동합니다. / 
이때 매번 미니배치는 무작위로 선정되기 때문에 / 이를 확률적 경사 하강법 줄여서 / SGD라고 부릅니다.

### 4.5.1 2층 신경망 클래스 구현하기

그러면 지금까지 학습한 신경망 모형의 구성에 대해 / 클래스로 구현해보도록 하겠습니다.

클래스의 이름은 해당 클래스의 구성을 알아보기 쉽게 / 2층 신경망이라는 이름으로 하고, / 해당 클래스를 선언할 때 입력 데이터의 크기, / 은닉 마디의 갯수, / 출력 데이터의 크기 등을 받습니다. / 
가중치에 대한 초기값은 임의의 값으로, 편향은 0으로 설정합니다. / 
해당 클래스는 predict 함수에서 / 가중치 및 편향과의 연산과 / 시그모이드 함수를 통해 다음 층으로 전달합니다. / 
가중치와의 연산을 한 번 더 진행하고 / softmax 함수를 통해 / 출력 클래스별로 확률값으로 바꿔 그 값을 함수 밖으로 보냅니다.

또, loss 함수에서 / 손실 함수 교차 엔트로피 오차법을 이용해 / 손실 함수 값을 구할 수 있습니다. / 
accuracy 함수를 통해 / 모형의 정확도를 계산해볼 수 있고 / numerical_gradient나

gradient 함수를 통해 / 학습 시 기울기 값을 계산하게 됩니다.

### 4.5.2 미니배치 학습 구현하기

이어서 이렇게 구현된 신경망을 학습시킬 때 이용할 / 미니배치 알고리즘에 대한 코드입니다. / 

해당 코드는 
iter_num과 Cost function의 함수를 보여주자.
# 계속 학습하면서 loss 값이 줄어들고 있음.

# 4.5.3 시험 데이터로 평가하기
# 앞선 loss는 훈련 데이터에 대한 오차 지표고 더 믿음직한 오류율을 구하고자
# 드디어 test 데이터를 사욯합니다.
# addition) epoch: 학습에서 훈련 데이터를 모두 소진했을 때의 횟수
# 10,000개를 100개의 미니배치로 학습하면 SGD를 100회 반복하면서 모든 훈련 데이터를 소진하게 됨. 이때 100회가 1에폭
