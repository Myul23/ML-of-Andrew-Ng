{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# fashion-mnist\n",
    "- [github](https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gzip\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(os.path.dirname(os.pardir)))))\n",
    "\n",
    "# data handling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mnist_test.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from mnist_test.optimizer import AdaGrad, Adam\n",
    "\n",
    "# data visulization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "#### 데이터 로딩 과정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    labels_path = os.path.join(path, \"%s-labels-idx1-ubyte.gz\" % kind)\n",
    "    images_path = os.path.join(path, \"%s-images-idx3-ubyte.gz\" % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                    offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist(\"C:\\\\Github Projects\\\\study_store\\\\Deep Learning Projects\\\\mini projects\\\\fashion_mnist\", kind='train')\n",
    "X_test, y_test = load_mnist(\"C:\\\\Github Projects\\\\study_store\\\\Deep Learning Projects\\\\mini projects\\\\fashion_mnist\", kind='t10k')\n",
    "# types = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "source": [
    "fashion data의 train input & output, test input & output을 확인\n",
    "<!-- 이.. os.path를 추가해도 path 제대로 못 읽는... -->"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 본격적으로\n",
    "- 2층 신경망 구현 -> 7층 신경망으로 확장 -> CNN\n",
    "- train & test / train & validation & test\n",
    "- dropout / batch size / max epoch\n",
    "- learning rate 범위 / weight decay 범위\n",
    "- batch normalization 필요한 작업이지만, 식이 너무 어"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### custom functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(data, label, times=3):\n",
    "    for i in range(times):\n",
    "        index = np.random.randint(data.shape[-2])\n",
    "        plt.subplot(1, times, i + 1)\n",
    "        plt.imshow(data[index].reshape(28, 28))\n",
    "        plt.title(label[index])\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "### validaion set\n",
    "- sklearn (scikit learn) -> model_selection -> train_test_split (-> split train & validation)\n",
    "- train_test_split(Xtrain, Ytrain, test_size=테스트비율, shuffle=셔플여부, ...)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train: (48000, 784) (48000,) \n test: (12000, 784) (12000,) \ntotal:  60000 \t   / 60000\n"
     ]
    }
   ],
   "source": [
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(\"train:\", X_train2.shape, y_train2.shape,\n",
    "      \"\\n test:\", X_val.shape, y_val.shape,\n",
    "      \"\\ntotal: \", X_train2.shape[0] + X_val.shape[0],\n",
    "      \"\\t   /\", y_train2.shape[0] + y_val.shape[0])"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show(X_train, y_train)"
   ]
  },
  {
   "source": [
    "### hyper-parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_ratio = 0.2\n",
    "batch_size = 100"
   ]
  },
  {
   "source": [
    "### Network 구성\n",
    "- Batch Normalization: True\n",
    "- Dropout: 0.2\n",
    "- Regularization: weight decay\n",
    "- Init Weight: False\n",
    "\n",
    "<br />\n",
    "\n",
    "- Hidden Layer: hidden layer num, hidden node size\n",
    "- Activation function: ReLU\n",
    "- Gradient Descent: AdaGrad, Adam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    learning_rate = 10 ** np.random.uniform(-3, -1)\n",
    "    weight_decay = 10 ** np.random.uniform(-4, -1)\n",
    "\n",
    "    network = MultiLayerNetExtend(\n",
    "        input_size=X_train.shape[-1], hidden_size_list=[100, 50, 100, 50, 100], output_size=10,\n",
    "        use_dropout=True, dropout_rate=dropout_ratio,\n",
    "        use_batchnorm=True, weight_decay_lambda=weight_decay)\n",
    "    optimizer = AdaGrad(lr=learning_rate)\n",
    "\n",
    "    # 모형 적합 시작\n",
    "    for j in range(1000):\n",
    "        batch_mask = np.random.choice(X_train2.shape[0], batch_size)\n",
    "        X_batch = X_train2[batch_mask]\n",
    "        y_batch = y_train2[batch_mask]\n",
    "\n",
    "        grads = network.gradient(X_batch, y_batch)\n",
    "        optimizer.update(network.params, grads)\n",
    "    print(f\"learning_rate: {learning_rate}, \\tweight_decay: {weight_decay}, \\tvalidation accuracy: {network.accuracy(X_val, y_val)}\")"
   ]
  },
  {
   "source": [
    "#### addition) 확장?\n",
    "\n",
    "<!-- accuracies = {}\n",
    "for i in range(100):\n",
    "    learning_rate = 10 ** np.random.uniform(-4, 1)\n",
    "    weight_decay = 10 ** np.random.uniform(-4, 0)\n",
    "\n",
    "    network = MultiLayerNetExtend(\n",
    "        input_size=X_train.shape[-1], hidden_size_list=[100, 50, 100, 50, 100], output_size=10,\n",
    "        use_dropout=True, dropout_rate=dropout_ratio,\n",
    "        use_batchnorm=True, weight_decay_lambda=weight_decay)\n",
    "    optimizer = AdaGrad(lr=learning_rate)\n",
    "\n",
    "    # 모형 적합 시작\n",
    "    for j in range(1000):\n",
    "        batch_mask = np.random.choice(X_train2.shape[0], batch_size)\n",
    "        X_batch = X_train2[batch_mask]\n",
    "        y_batch = y_train2[batch_mask]\n",
    "\n",
    "        grads = network.gradient(X_batch, y_batch)\n",
    "        optimizer.update(network.params, grads)\n",
    "    accu = network.accuracy(X_val, y_val)\n",
    "    accuracies[\"learning_rate: \" + str(learning_rate) + \"\\tweight_decay: \" + str(weight_decay)] = accu\n",
    "    # print(f\"learning_rate: {learning_rate}, \\tweight_decay: {weight_decay}, \\tvalidation accuracy: {network.accuracy(X_val, y_val)}\")\n",
    "\n",
    "accu_sorted = sorted(accuracies.items(), key=lambda w:w[1], reverse=True)\n",
    "for key, value in acc_sorted:\n",
    "    print(key, \"\\tvalidation accuracy:\", value) -->\n",
    "\n",
    "```\n",
    "for opt in [AdaGrad, Adam]:\n",
    "    hypa = []\n",
    "    accuracy = []\n",
    "    for i in range(100):\n",
    "        learning_rate = 10 ** np.random.uniform(-3, 3)\n",
    "        weight_decay = 10 ** np.random.uniform(-4, 4)\n",
    "        \n",
    "        network = MultiLayerNetExtend(\n",
    "            input_size=X_train.shape[-1], hidden_size_list=[100, 50, 100, 50, 100], output_size=10,\n",
    "            use_dropout=True, dropout_rate=dropout_ratio,\n",
    "            use_batchnorm=True, weight_decay_lambda=weight_decay)\n",
    "        optimizer = opt(lr=learning_rate)\n",
    "\n",
    "        # 모형 적합 시작\n",
    "        for j in range(1000):\n",
    "            batch_mask = np.random.choice(X_train2.shape[0], batch_size)\n",
    "            X_batch = X_train2[batch_mask]\n",
    "            y_batch = y_train2[batch_mask]\n",
    "\n",
    "            grads = network.gradient(X_batch, y_batch)\n",
    "            optimizer.update(network.params, grads)\n",
    "        \n",
    "        hypa.append([learning_rate, weight_decay])\n",
    "        accu = network.accuracy(X_val, y_val)\n",
    "        accuracy.append(accu)\n",
    "    \n",
    "    # 여기서 Best 찾기\n",
    "    max_key = max(accuracy.index, key=lambda w:accuracy[w])\n",
    "    print(str(opt), \"\\t\" + max_key, \"\\tvalidation accuracy:\", accuracy[max_key])\n",
    "# print(f\"learning_rate: {learning_rate}, \\tweight_decay: {weight_decay}, \\tvalidation accuracy: {network.accuracy(X_val, y_val)}\")\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### output 확인"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(network.predict(X_train), axis=1)\n",
    "img_show(X_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(network.predict(X_test), axis=1)\n",
    "img_show(X_test, y_pred)\n",
    "print(network.accuracy(X_test, y_test))"
   ]
  },
  {
   "source": [
    "### 다른 모형을 적합해보자."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_test.trainer import Trainer\n",
    "\n",
    "def __train(network, x_train, t_train, x_val, t_val, lr, opt=\"sgd\", epocs=50):\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "        epochs=epocs, mini_batch_size=100, optimizer=opt, optimizer_param={\"lr\": lr},\n",
    "        verbose=False)\n",
    "    trainer.train()\n",
    "    # return trainer.test_acc_list, trainer.train_acc_list"
   ]
  },
  {
   "source": [
    "제대로 작동하는지 확인"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "learning_rate: 0.010435555614413346, \tweight_decay: 0.0016788230303147903, \tvalidation accuracy: 0.8045833333333333\n",
      "learning_rate: 0.0013239241176477784, \tweight_decay: 0.08158533834432241, \tvalidation accuracy: 0.60175\n",
      "learning_rate: 0.005526934025531127, \tweight_decay: 0.016823037262989535, \tvalidation accuracy: 0.7209166666666667\n",
      "learning_rate: 0.0024453573661994093, \tweight_decay: 0.05925329795668869, \tvalidation accuracy: 0.71925\n",
      "learning_rate: 0.08557023165840626, \tweight_decay: 0.0049542313575567576, \tvalidation accuracy: 0.655\n",
      "learning_rate: 0.014484737619278496, \tweight_decay: 0.008307400028009474, \tvalidation accuracy: 0.7306666666666667\n",
      "learning_rate: 0.03198670451395344, \tweight_decay: 0.013059108617018691, \tvalidation accuracy: 0.64475\n",
      "learning_rate: 0.005056321955187653, \tweight_decay: 0.003714618085498175, \tvalidation accuracy: 0.7746666666666666\n",
      "learning_rate: 0.011741339176727296, \tweight_decay: 0.011728384469747897, \tvalidation accuracy: 0.7709166666666667\n",
      "learning_rate: 0.019835614226669986, \tweight_decay: 0.005171808865538309, \tvalidation accuracy: 0.7260833333333333\n",
      "learning_rate: 0.06697402064937605, \tweight_decay: 0.0806458064359064, \tvalidation accuracy: 0.23758333333333334\n",
      "learning_rate: 0.017814640484134524, \tweight_decay: 0.0007357535758089947, \tvalidation accuracy: 0.7983333333333333\n",
      "learning_rate: 0.03140501078668399, \tweight_decay: 0.0006747926638653836, \tvalidation accuracy: 0.7895833333333333\n",
      "learning_rate: 0.05221396102747698, \tweight_decay: 0.02642372701844307, \tvalidation accuracy: 0.4718333333333333\n",
      "learning_rate: 0.02663310126350704, \tweight_decay: 0.0006514617040483755, \tvalidation accuracy: 0.82075\n",
      "learning_rate: 0.013109920863726872, \tweight_decay: 0.0005698830051773992, \tvalidation accuracy: 0.7889166666666667\n",
      "learning_rate: 0.004210353973125461, \tweight_decay: 0.0004801202751163569, \tvalidation accuracy: 0.765\n",
      "learning_rate: 0.0021487661197053076, \tweight_decay: 0.09793290097171618, \tvalidation accuracy: 0.67225\n",
      "learning_rate: 0.0028373601003204643, \tweight_decay: 0.00022179318067556814, \tvalidation accuracy: 0.7398333333333333\n",
      "learning_rate: 0.07063798074703728, \tweight_decay: 0.00036878293376675414, \tvalidation accuracy: 0.7645\n",
      "learning_rate: 0.009960853579454735, \tweight_decay: 0.07940092945688142, \tvalidation accuracy: 0.36241666666666666\n",
      "learning_rate: 0.03286248219989795, \tweight_decay: 0.01091102987075638, \tvalidation accuracy: 0.6865\n",
      "learning_rate: 0.008712136598105352, \tweight_decay: 0.006443158860660287, \tvalidation accuracy: 0.7904166666666667\n",
      "learning_rate: 0.04142909615311624, \tweight_decay: 0.0004085359913169329, \tvalidation accuracy: 0.80275\n",
      "learning_rate: 0.02751560348950653, \tweight_decay: 0.002960780581775637, \tvalidation accuracy: 0.7495833333333334\n",
      "learning_rate: 0.010703944253118049, \tweight_decay: 0.02032576820847049, \tvalidation accuracy: 0.7331666666666666\n",
      "learning_rate: 0.00488731214790913, \tweight_decay: 0.00851237257828837, \tvalidation accuracy: 0.7706666666666667\n",
      "learning_rate: 0.06702630397924808, \tweight_decay: 0.03235433082175433, \tvalidation accuracy: 0.37825\n",
      "learning_rate: 0.01387707806044695, \tweight_decay: 0.005414487956770877, \tvalidation accuracy: 0.7914166666666667\n",
      "learning_rate: 0.011714885469045417, \tweight_decay: 0.009764220158405125, \tvalidation accuracy: 0.7785\n",
      "learning_rate: 0.0039404501546667645, \tweight_decay: 0.0808917658623563, \tvalidation accuracy: 0.50275\n",
      "learning_rate: 0.006856956790106827, \tweight_decay: 0.00027787267522418106, \tvalidation accuracy: 0.7504166666666666\n",
      "learning_rate: 0.07721817219324177, \tweight_decay: 0.02573686047471643, \tvalidation accuracy: 0.1855\n",
      "learning_rate: 0.05731438024864634, \tweight_decay: 0.01665128517575375, \tvalidation accuracy: 0.43366666666666664\n",
      "learning_rate: 0.001782596327440516, \tweight_decay: 0.0002898567917015984, \tvalidation accuracy: 0.6863333333333334\n",
      "learning_rate: 0.029142011181152068, \tweight_decay: 0.00011194733275039235, \tvalidation accuracy: 0.8204166666666667\n",
      "learning_rate: 0.001180192186117482, \tweight_decay: 0.00034520827114902327, \tvalidation accuracy: 0.6779166666666666\n",
      "learning_rate: 0.02451542581291957, \tweight_decay: 0.004610927741575478, \tvalidation accuracy: 0.79925\n",
      "learning_rate: 0.002308953505533353, \tweight_decay: 0.0011636364174184154, \tvalidation accuracy: 0.7385833333333334\n",
      "learning_rate: 0.003999912231934378, \tweight_decay: 0.010555163116444399, \tvalidation accuracy: 0.76225\n",
      "learning_rate: 0.004406543494099883, \tweight_decay: 0.030875342847548698, \tvalidation accuracy: 0.7285\n",
      "learning_rate: 0.04504737453643148, \tweight_decay: 0.004420594491592507, \tvalidation accuracy: 0.7509166666666667\n",
      "learning_rate: 0.04506738459240551, \tweight_decay: 0.006779971228549623, \tvalidation accuracy: 0.6005833333333334\n",
      "learning_rate: 0.01606845633887079, \tweight_decay: 0.0010908072883410247, \tvalidation accuracy: 0.8039166666666666\n",
      "learning_rate: 0.03668620179471998, \tweight_decay: 0.011765420871421729, \tvalidation accuracy: 0.6131666666666666\n",
      "learning_rate: 0.0019325026638366296, \tweight_decay: 0.003945865266549996, \tvalidation accuracy: 0.6491666666666667\n",
      "learning_rate: 0.005270434600704128, \tweight_decay: 0.00020069918860022827, \tvalidation accuracy: 0.7975833333333333\n",
      "learning_rate: 0.020057752261507886, \tweight_decay: 0.005217140067604226, \tvalidation accuracy: 0.758\n",
      "learning_rate: 0.05908186362060342, \tweight_decay: 0.0012303317166518827, \tvalidation accuracy: 0.7668333333333334\n",
      "learning_rate: 0.029153430540662086, \tweight_decay: 0.00011683921422568345, \tvalidation accuracy: 0.7985\n",
      "learning_rate: 0.0010269995972421668, \tweight_decay: 0.005143638996708584, \tvalidation accuracy: 0.63525\n",
      "learning_rate: 0.001014070331170656, \tweight_decay: 0.02516620485131371, \tvalidation accuracy: 0.6723333333333333\n",
      "learning_rate: 0.0032235538270057105, \tweight_decay: 0.005173261635110375, \tvalidation accuracy: 0.7571666666666667\n",
      "learning_rate: 0.0012105440681097843, \tweight_decay: 0.08521863091742496, \tvalidation accuracy: 0.6938333333333333\n",
      "learning_rate: 0.0018439951634177704, \tweight_decay: 0.07762839750831786, \tvalidation accuracy: 0.68175\n",
      "learning_rate: 0.0016385924241205116, \tweight_decay: 0.011670040093481314, \tvalidation accuracy: 0.6664166666666667\n",
      "learning_rate: 0.0872573553231402, \tweight_decay: 0.0919528817944037, \tvalidation accuracy: 0.179\n",
      "learning_rate: 0.009543971455882951, \tweight_decay: 0.03822447757760008, \tvalidation accuracy: 0.6454166666666666\n",
      "learning_rate: 0.0021438309312744844, \tweight_decay: 0.0004627197905225593, \tvalidation accuracy: 0.73075\n",
      "learning_rate: 0.042593662037222216, \tweight_decay: 0.0012199987766788214, \tvalidation accuracy: 0.7990833333333334\n",
      "learning_rate: 0.0063858024970652166, \tweight_decay: 0.0029979838395652876, \tvalidation accuracy: 0.79475\n",
      "learning_rate: 0.014311016377366469, \tweight_decay: 0.0007362948437486106, \tvalidation accuracy: 0.7865833333333333\n",
      "learning_rate: 0.006828629907639374, \tweight_decay: 0.0006765419056788952, \tvalidation accuracy: 0.8035833333333333\n",
      "learning_rate: 0.08262361333780889, \tweight_decay: 0.09288701939628467, \tvalidation accuracy: 0.25933333333333336\n",
      "learning_rate: 0.03668481505098731, \tweight_decay: 0.034097330314916054, \tvalidation accuracy: 0.32075\n",
      "learning_rate: 0.009950580853115532, \tweight_decay: 0.021039707981941397, \tvalidation accuracy: 0.6895833333333333\n",
      "learning_rate: 0.0010587838009214314, \tweight_decay: 0.06938719746878079, \tvalidation accuracy: 0.72325\n",
      "learning_rate: 0.020077208731406802, \tweight_decay: 0.021086914528651923, \tvalidation accuracy: 0.6635833333333333\n",
      "learning_rate: 0.0013790403690316908, \tweight_decay: 0.00558255458703641, \tvalidation accuracy: 0.7161666666666666\n",
      "learning_rate: 0.007979326806585642, \tweight_decay: 0.06426604298534258, \tvalidation accuracy: 0.5214166666666666\n",
      "learning_rate: 0.0010105719253969364, \tweight_decay: 0.006050890954752923, \tvalidation accuracy: 0.6930833333333334\n",
      "learning_rate: 0.03650385232662355, \tweight_decay: 0.0002800601383484526, \tvalidation accuracy: 0.81375\n",
      "learning_rate: 0.0027564512459525388, \tweight_decay: 0.00011855816433667916, \tvalidation accuracy: 0.7075833333333333\n",
      "learning_rate: 0.002397654745679349, \tweight_decay: 0.07218979335860806, \tvalidation accuracy: 0.64075\n",
      "learning_rate: 0.08410131140813445, \tweight_decay: 0.0008528081437570788, \tvalidation accuracy: 0.7868333333333334\n",
      "learning_rate: 0.0014065197662936624, \tweight_decay: 0.0007058264163329876, \tvalidation accuracy: 0.6571666666666667\n",
      "learning_rate: 0.024506207220081032, \tweight_decay: 0.0005730166177047555, \tvalidation accuracy: 0.8251666666666667\n",
      "learning_rate: 0.003348451362822505, \tweight_decay: 0.00010342856799632285, \tvalidation accuracy: 0.76625\n",
      "learning_rate: 0.02903125093275985, \tweight_decay: 0.0007412529128328872, \tvalidation accuracy: 0.8035833333333333\n",
      "learning_rate: 0.0025660030520950204, \tweight_decay: 0.008326484292588629, \tvalidation accuracy: 0.6934166666666667\n",
      "learning_rate: 0.027060350680395598, \tweight_decay: 0.0007526115067104144, \tvalidation accuracy: 0.8276666666666667\n",
      "learning_rate: 0.029245728856555908, \tweight_decay: 0.04820393801668717, \tvalidation accuracy: 0.43925\n",
      "learning_rate: 0.002279497169740386, \tweight_decay: 0.0002442902559375748, \tvalidation accuracy: 0.7478333333333333\n",
      "learning_rate: 0.010292571635419329, \tweight_decay: 0.022846751875663904, \tvalidation accuracy: 0.7701666666666667\n",
      "learning_rate: 0.05206985826619936, \tweight_decay: 0.09507141267286918, \tvalidation accuracy: 0.19241666666666668\n",
      "learning_rate: 0.04662885418611058, \tweight_decay: 0.04815894137674513, \tvalidation accuracy: 0.09975\n",
      "learning_rate: 0.001150988382225631, \tweight_decay: 0.0030523607269927937, \tvalidation accuracy: 0.6541666666666667\n",
      "learning_rate: 0.021430530400787316, \tweight_decay: 0.0008547127229740044, \tvalidation accuracy: 0.7928333333333333\n",
      "learning_rate: 0.0325633795056, \tweight_decay: 0.0004394078010102671, \tvalidation accuracy: 0.8036666666666666\n",
      "learning_rate: 0.08473968380953076, \tweight_decay: 0.023493167832967857, \tvalidation accuracy: 0.38658333333333333\n",
      "learning_rate: 0.058868585947794025, \tweight_decay: 0.021623890511986123, \tvalidation accuracy: 0.4979166666666667\n",
      "learning_rate: 0.07034293163742111, \tweight_decay: 0.00011424134776451049, \tvalidation accuracy: 0.79575\n",
      "learning_rate: 0.005964869491635683, \tweight_decay: 0.006950573918008708, \tvalidation accuracy: 0.7625833333333333\n",
      "learning_rate: 0.0011178711570385485, \tweight_decay: 0.024911380622014517, \tvalidation accuracy: 0.6174166666666666\n",
      "learning_rate: 0.019010421487438478, \tweight_decay: 0.04191643906472812, \tvalidation accuracy: 0.4156666666666667\n",
      "learning_rate: 0.0013818435813792696, \tweight_decay: 0.018364458124556556, \tvalidation accuracy: 0.6480833333333333\n",
      "learning_rate: 0.002713280642821223, \tweight_decay: 0.0591967088472386, \tvalidation accuracy: 0.7275\n",
      "learning_rate: 0.010320863942300628, \tweight_decay: 0.0006890605241149176, \tvalidation accuracy: 0.8139166666666666\n",
      "learning_rate: 0.0010139907267888767, \tweight_decay: 0.0025067264921108928, \tvalidation accuracy: 0.6624166666666667\n",
      "learning_rate: 0.0025463593681544143, \tweight_decay: 0.0029159155121012705, \tvalidation accuracy: 0.6774166666666667\n",
      "0.6765333333333333\n",
      "0.6721\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    learning_rate = 10 ** np.random.uniform(-3, -1)\n",
    "    weight_decay = 10 ** np.random.uniform(-4, -1)\n",
    "\n",
    "    network = MultiLayerNetExtend(\n",
    "        input_size=X_train.shape[-1], hidden_size_list=[100, 50, 100, 50, 100], output_size=10,\n",
    "        use_dropout=True, dropout_rate=dropout_ratio,\n",
    "        activation=\"sigmoid\", weight_init_std=\"sigmoid\",\n",
    "        use_batchnorm=True, weight_decay_lambda=weight_decay)\n",
    "    optimizer = AdaGrad(lr=learning_rate)\n",
    "\n",
    "    # 모형 적합 시작\n",
    "    for j in range(1000):\n",
    "        batch_mask = np.random.choice(X_train2.shape[0], batch_size)\n",
    "        X_batch = X_train2[batch_mask]\n",
    "        y_batch = y_train2[batch_mask]\n",
    "\n",
    "        grads = network.gradient(X_batch, y_batch)\n",
    "        optimizer.update(network.params, grads)\n",
    "    print(f\"learning_rate: {learning_rate}, \\tweight_decay: {weight_decay}, \\tvalidation accuracy: {network.accuracy(X_val, y_val)}\")\n",
    "\n",
    "y_pred = np.argmax(network.predict(X_train), axis=1)\n",
    "print(network.accuracy(X_train, y_train))\n",
    "y_pred = np.argmax(network.predict(X_test), axis=1)\n",
    "print(network.accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 10 ** np.random.uniform(-3, -1)\n",
    "weight_decay = 10 ** np.random.uniform(-4, -1)\n",
    "\n",
    "network = MultiLayerNetExtend(\n",
    "    input_size=X_train.shape[-1], hidden_size_list=[100, 50, 100, 50, 100], output_size=10,\n",
    "    use_dropout=True, dropout_rate=dropout_ratio,\n",
    "    use_batchnorm=True, weight_decay_lambda=weight_decay)\n",
    "__train(network, x_train=X_train2, t_train=y_train2, x_val=X_val, t_val=y_val, lr=learning_rate, opt=\"adagrad\")\n",
    "\n",
    "print(f\"learning_rate: {learning_rate}, \\tweight_decay: {weight_decay}, \\tvalidation accuracy: {network.accuracy(X_val, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    learning_rate = 10 ** np.random.uniform(-3, -1)\n",
    "    weight_decay = 10 ** np.random.uniform(-4, -1)\n",
    "\n",
    "    network = MultiLayerNetExtend(\n",
    "        input_size=X_train.shape[-1], hidden_size_list=[100, 50, 100, 50, 100], output_size=10,\n",
    "        use_dropout=False, dropout_rate=dropout_ratio,\n",
    "        use_batchnorm=True, weight_decay_lambda=weight_decay)\n",
    "\n",
    "    __train(network, x_train=X_train2, t_train=y_train2, x_val=X_val, t_val=y_val, lr=learning_rate, opt=\"adam\")\n",
    "\n",
    "    print(f\"learning_rate: {learning_rate}, \\tweight_decay: {weight_decay}, \\tvalidation accuracy: {network.accuracy(X_val, y_val)}\")\n",
    "\n",
    "y_pred = np.argmax(network.predict(X_test), axis=1)\n",
    "print(network.accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(network.predict(X_train), axis=1)\n",
    "print(network.accuracy(X_train, y_train))\n",
    "y_pred = np.argmax(network.predict(X_test), axis=1)\n",
    "print(network.accuracy(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}