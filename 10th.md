> # Machine Learning

- Instructor: Andrew Ng
- Lectures: [Coursera](https://www.coursera.org/learn/machine-learning?action=enroll)
- [Sub-link](https://www.coursera.org/lecture/machine-learning/model-representation-db3jS?utm_source=link&utm_medium=in_course_lecture&utm_content=page_share&utm_campaign=overlay_button)

---

#### Why prefer using Large Dataset

- sample은 비용적 측면에서 환영해야 할 기법이지만, 슬프게도 표본 오류를 피해갈 수 없다.
- 이 때문에 가능하다면 larger data를, 모집단을 이용하려고 하는 것이다.
- 잘못된, 편향된 표본은 그 결과를 신뢰할 수 없다.


#### Batch gradient descent

- 기존의 Gradient Descent는 large data를 적합시킬 때 O(n^3) 정도로 시간이 너무 오래 걸린다.
- 이를 보완하고 validation set을 이용한 parameter 조정을 한 번에 하게 한다.

1. randomly shuffle(reorder) dataset. (물론 test dataset은 나눠둬야 하겠지만)
2. batch_size에 대해 (부분) Gradient Descent를 반복한다.

- Stochastic gradient descent(확률적 경사 하강법): 단일 학습 샘플을 이용한다.
- Mini-batch gradient descent: 연속된 data를 가져오는 게 아니라 일정한 간격을 가진 데이터를 추출해 Batch Gradient Descent하는 방법(대신 reorder는 optional이 되지 않을까)
- Mini-batch만 하고 Batch Gradient Descent를 하지 않는 건 b = m인 Batch Gradient Descent를 하는 것과 같다.

---

- Online Learning: Website Learning Alogithm using user's data
- It can adapt to changing user tases (changes over time), It allows us to learn from a continuous stream of data, since we use each example once then no longer need to precess it again.
- 운송 수단 사용과 검색한 정보에 가까운 휴대폰 추천 등의 알고리즘을 설명하셨지만, x로 정의되는 것들과 특수하게 인식 및 다뤄야 하는 것들만 이해하고 넘겼다.

---

#### Map-reduce

- Batch를 이용한 방법이 한 번에 한 뭉치만 Gradient descent를 통한 계수 조정을 하는 거라면, Map-reduce는 Gradient descent뿐만 아니라 모델 자체를 Batch를 통해 돌리는 것이다.
- 이후, 전체적인 모델을 구성할 때는 위에 계수들을 GAM처럼 더한 후 조정 및 이용하게 된다.
- 설마 이게 HADOOP? 비슷한 개념이긴 한데, HADOOP 정의가 이런 것인가?
